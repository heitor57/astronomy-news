\section{Introdução}
O ato de baixar automaticamente os dados de uma página web, extrair os hiperlinks contidos nela e segui-los é conhecido como web crawling. Os dados baixados são geralmente armazenados em um índice ou banco de dados para facilitar sua busca. Web crawler, também conhecido como indexação, é usado para indexar as informações em uma página web usando robôs, também chamados de crawlers. Web Crawlers são utilizados pelos principais motores de busca como o Google, Bing e Yahoo.

Por sua vez, o ato de baixar automaticamente os dados de uma página web e extrair informações específicas dela é intitulado como Web scraping. As informações extraídas podem ser armazenadas por diferentes meios (banco de dados, arquivo, etc.). Web scraping, também conhecido como extração de dados da web, é uma maneira automatizada de extrair informações/conteúdo usando robôs, conhecidos como scrapers. As informações extraídas podem ser usadas de diversas maneiras, como por exemplo para reunir em um site dados de determinado domínio obtidos de páginas do mesmo domínio na web e também podem ser usadas para análise de dados possibilitando obtenção de informações valiosas.

Na internet existem diversos pontos finais que distribuem informações não redundantes, a informação é espalhada de forma que um usuário precisa acessar cada servidor para obter informações. Em portais de notícias, muitas das vezes um leitor assíduo de um determinado assunto se vê na necessidade de acessar diversos portais para inteirar-se de uma discussão através de diferentes pontos de vista ou para ficar à par de todas as novidades naquele domínio, dado que um portal pode cobrir um acontecimento que os outros portais não cobriram. Neste sentido este trabalho propõe a centralização de todas as notícias publicadas nos principais portais de notícias brasileiros acerca de um tema em uma única aplicação.

Diante da contextualização este trabalho tem por objetivo o desenvolvimento de um coletor de dados referentes ao domínio “astronomia/cosmos/espaço” em portais conceituados de notícias brasileiros utilizando técnicas de web scraping e web crawler. Para armazenar esses dados, será feito o uso de um banco de dados NoSQL (MongoDB), que se mostra interessante para o mini-mundo diante da modelagem conceitual elaborada e do nosso conhecimento prévio para alto desempenho, baixa redûndancia, desenvolvimento ágil, maior facilidade no manuseamento e evolução das diferentes formas de dados, e principalmente o suporte a big data o MongoDB será essencial nas fases posteriores de: recuperação de informação; aplicação de técnicas de mineração de dados e etc. A partir desse banco de dados, será desenvolvido uma aplicação web que permitirá que usuários diversos façam as mais variadas consultas a esses dados.

Neste documento, referente à primeira etapa do trabalho, será descrito tecnicamente o coletor e o modelo conceitual do banco de dados.
