
\section{Coletor De Dados}

\subsection{Crawler}
Procurou-se pela maior gama possível de portais de notícias brasileiros relevantes que possuíam uma sessão destinada a assuntos relacionados à astronomia para serem varridos e indexados pelo crawler.

A Tabela 1 mostra os portais e suas respectivas seeds na qual o crawler irá realizar a varredura completa em busca dos links contidos a partir da seed.


\begin{table}
\begin{tabular}{|c|c|}
\hline
Domínio & Seed\\\hline
TecMundo & tecmundo.com.br/astronomia \\\hline
BBC & bbc.com/portuguese/topics/ckdxnd38v03t\\\hline
UOL & uol.com.br/tilt/astronomia/\\\hline
R7 & noticias.r7.com/tecnologia-e-ciencia/astronomia \\\hline
Veja & veja.abril.com.br/noticias-sobre/astronomia/ \\\hline
IG & ultimosegundo.ig.com.br/noticias/espaço \\\hline
Terra & terra.com.br/noticias/ciencia/espaco/\\\hline
Jornal USP & jornal.usp.br/tag/astronomia/\\\hline
Revista Galileu & revistagalileu.globo.com/Ciencia/Espaco/\\\hline
Exame & exame.com/noticias-sobre/espaco/\\\hline
CanalTech & canaltech.com.br/espaco/\\\hline
\end{tabular}
\caption{Seeds referentes aos conteúdos de astronomia para cada domínio.}
\end{table}


\subsection{Scraping}
Para cada domínio em questão, foram mapeados os dados que seriam possíveis de serem coletados pelo seu respectivo algoritmo coletor.
 
O mapeamento foi realizado de acordo com a seguinte metodologia: para cada domínio, foram acessados no mínimo cinco publicações aleatórias (link para as publicações) e em cada uma delas anotado quais componentes apareciam de maneira coletável em seu código HTML. Se um componente aparecer ao menos uma vez em alguma das páginas acessadas do domínio, será implementado no algoritmo de coleta de dados, para este domínio, uma rotina que irá extrair os dados deste componente sempre que o mesmo estiver contido no HTML da publicação. A Tabela 2 apresenta as informações obtidas a partir destas ponderações.  

\begin{table}
\noindent\makebox[\textwidth]{%
\input{content_table_1.tex}}
\caption{Dados possíveis de serem coletados nas publicações de cada domínio.}
\end{table}

De acordo com a Tabela 2, pode-se observar que seis dados são possíveis de extrair da página para todos os domínios, são eles: título, descrição (texto chamada da matéria), imagem principal (imagem de chamada da matéria), data (última modificação na publicação), conteúdo (o corpo da matéria em si) e o autor/escritor da publicação. Este último contém peculiaridades, visto que algumas publicações não especificam unitariamente quem é o autor, como em casos nos quais a autoria da publicação é destinada à “Redação”. Pretende-se utilizar os dados dos autores em uma funcionalidade extra na aplicação final, porém nesta funcionalidade informações de autoria como “Redação” não terão serventia, sendo necessário uma filtragem nos dados deste tipo.

Ainda sobre a Tabela 2, poucos domínios informam o tempo de leitura em suas publicações, apenas dois. Mesmo assim, foi optado por armazenar este dado no banco de dados, pois estuda-se desenvolver um algoritmo que estime o tempo de leitura para os domínios que não provêm esta informação. A respeito dos comentários, alguns domínios oferecem esta funcionalidade aos leitores. Para os domínios que oferecem, existem àqueles que usam o plugin Disqus, àqueles que usam o plugin do Facebook e àqueles que implementaram o seu próprio sistema de comentários, sendo necessário diferentes métodos para coletar estas informações. Nos sites que usam o sistema do Disqus existe ainda a funcionalidade “Reação” que permite o usuário reagir à publicação com um emoji.

Finalizando o estudo da Tabela 2, grande parte dos domínios dispõe, em algum ponto do código HTML, de um conjunto de palavras chaves, “tags”, que na aplicação a ser desenvolvida servirá de classificação dos sub-temas das publicações.

Para a coleta do conteúdo da matéria em si (o corpo da publicação), deve ser levado em consideração quais elementos podem figurar entre os parágrafos textuais da publicação. O caminho fácil seria coletar somente os textos, porém foi uma decisão de projeto manter o máximo possível da integridade e originalidade do conteúdo exposto na publicação, tornando-se necessário observar também o que pode estar contido no meio do conteúdo textual da matéria. 

Sendo assim, a mesma metodologia descrita anteriormente foi utilizada para o processo de análise e reconhecimento dos conteúdos das publicações de cada domínio e como deve ser implementado o seu respectivo coletor. As informações obtidas destas observações são apresentadas na Tabela 3.  


%A linguagem de programação utilizada para a implementação dos coletores de dados será o Python, com o auxílio da biblioteca requests e requests-html para realizar as requisições GET das URLs indexadas pelo crawler e da biblioteca BeautifulSoup para acessar os ‘nós’ da estrutura do HTML da página ou até mesmo classes e extrair as informações nelas contidas.

%Tomando como exemplo o domínio TecMundo, através da análise da estrutura do HTML das publicações contidas na seed “tecmundo.com.br/astronomia/” o coletor de dados pode ser projetado da forma descrita a seguir:

%Dada uma URL indexada pelo crawler para este domínio, realiza-se uma requisição GET, e obtém-se o código HTML da página provida pela URL. Este código HTML é convertido para um objeto da classe BeautifulSoup e armazenado em uma variável (soup) para que possa ser realizado a busca e extração de dados dentro do DOM.

%Em seguida, começa-se a coleta de dados com a função find() da biblioteca BeautifulSoup.

%O título da matéria é extraído com a seguinte declaração:

%soup.find('meta', property='og:title').get('content')


%Nesta declaração, estamos buscando por um elemento HTML <meta> com um atributo property que recebe o valor ‘og:title’, deste elemento, queremos o valor contido no atributo ‘content’.

%O nome do colunista é obtido pela declaração abaixo: 

%soup.find(a, class_='tec--author__info__link').string


%Na declaração acima, procuramos por um elemento <a> que contém a classe ‘tec--author__info__link’, do elemento encontrado queremos a string dentro do elemento <a>.

%O BeautifulSoup permite filtrar as buscas por um determinado dado por meio de atributos, classes e id's que casam com a estrutura procurada, como feito nas declarações mostradas anteriormente. Porém nem sempre o dado que se quer extrair está contido em um elemento que possua algo que o diferencia dos restantes, nestes casos é adicionado uma complexidade a mais na coleta destes dados.

